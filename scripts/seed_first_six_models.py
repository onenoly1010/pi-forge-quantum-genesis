#!/usr/bin/env python3
"""
Seed First Six MR-NFT Models
Deploys the initial ethical AI validation models to Pi Network

This script is part of the OINIO succession ceremony and handles
deployment of the six foundational MR-NFT models to Pi Network.

Usage:
    # Dry run (testnet simulation)
    python scripts/seed_first_six_models.py --dry-run
    
    # Deploy single model
    python scripts/seed_first_six_models.py --model "Ethics Validator" --execute
    
    # Deploy all six models (requires confirmation)
    python scripts/seed_first_six_models.py --execute-all --confirm
    
    # Deploy with custom gas limit
    python scripts/seed_first_six_models.py --execute-all --confirm --gas-limit 3000000

For detailed deployment guide, see: docs/DEPLOYMENT_CHECKLIST.md
"""

import os
import sys
import json
import time
import argparse
from datetime import datetime
from typing import Dict, List, Optional

# Check for required dependencies
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    print("‚ö†Ô∏è  python-dotenv not installed. Using system environment variables.")

# Model definitions - The six seed models
SEED_MODELS = [
    {
        "name": "Ethics Validator",
        "description": "Validates AI systems against multi-dimensional ethics frameworks including fairness, transparency, accountability, and societal impact",
        "model_id": "oinio/ethics-validator-v1",
        "royalty_rate": 0.15,  # 15%
        "complexity": "standard",
        "deployment_cost": 120_000,  # PI
        "category": "ethics",
        "version": "1.0.0"
    },
    {
        "name": "Bias Detector",
        "description": "Detects demographic and systemic bias in AI models across multiple dimensions including race, gender, age, and socioeconomic factors",
        "model_id": "oinio/bias-detector-v1",
        "royalty_rate": 0.20,  # 20%
        "complexity": "complex",
        "deployment_cost": 150_000,  # PI
        "category": "bias",
        "version": "1.0.0"
    },
    {
        "name": "Privacy Auditor",
        "description": "Audits data handling and privacy compliance against GDPR, CCPA, and other global privacy regulations",
        "model_id": "oinio/privacy-auditor-v1",
        "royalty_rate": 0.15,  # 15%
        "complexity": "standard",
        "deployment_cost": 120_000,  # PI
        "category": "privacy",
        "version": "1.0.0"
    },
    {
        "name": "Transparency Scorer",
        "description": "Scores model explainability and decision transparency using multiple interpretability techniques",
        "model_id": "oinio/transparency-scorer-v1",
        "royalty_rate": 0.10,  # 10%
        "complexity": "simple",
        "deployment_cost": 80_000,  # PI
        "category": "transparency",
        "version": "1.0.0"
    },
    {
        "name": "Fairness Analyzer",
        "description": "Analyzes outcome fairness across user groups and identifies disparate impact in AI system decisions",
        "model_id": "oinio/fairness-analyzer-v1",
        "royalty_rate": 0.20,  # 20%
        "complexity": "complex",
        "deployment_cost": 150_000,  # PI
        "category": "fairness",
        "version": "1.0.0"
    },
    {
        "name": "Accountability Tracker",
        "description": "Tracks decision lineage and responsibility chains throughout AI system operations for complete accountability",
        "model_id": "oinio/accountability-tracker-v1",
        "royalty_rate": 0.30,  # 30% (highest value, most complex)
        "complexity": "premium",
        "deployment_cost": 200_000,  # PI
        "category": "accountability",
        "version": "1.0.0"
    }
]


class ModelDeployer:
    """Handles deployment of MR-NFT models to Pi Network"""
    
    def __init__(self, dry_run: bool = False, gas_limit: Optional[int] = None):
        self.dry_run = dry_run
        self.gas_limit = gas_limit or 2_000_000
        self.deployment_log = []
        
        # Load configuration from environment
        self.pi_network_mode = os.getenv('PI_NETWORK_MODE', 'testnet')
        self.oinio_wallet = os.getenv('OINIO_WALLET_ADDRESS')
        self.factory_contract = os.getenv('MRNFT_FACTORY_ADDRESS')
        
        # Validate configuration
        self._validate_config()
        
    def _validate_config(self):
        """Validate environment configuration"""
        if not self.dry_run:
            if not self.oinio_wallet:
                raise ValueError("OINIO_WALLET_ADDRESS environment variable required")
            if not self.factory_contract:
                raise ValueError("MRNFT_FACTORY_ADDRESS environment variable required")
            
            if self.pi_network_mode == 'mainnet':
                print("‚ö†Ô∏è  MAINNET MODE - Real PI will be spent!")
                print(f"   Wallet: {self.oinio_wallet}")
                print(f"   Factory: {self.factory_contract}")
            else:
                print(f"‚úÖ Using {self.pi_network_mode.upper()} mode")
    
    def prepare_model_metadata(self, model: Dict) -> Dict:
        """
        Prepare model metadata for on-chain deployment
        
        Args:
            model: Model definition dictionary
            
        Returns:
            Metadata dictionary ready for deployment
        """
        metadata = {
            "name": model["name"],
            "description": model["description"],
            "model_id": model["model_id"],
            "version": model["version"],
            "category": model["category"],
            "complexity": model["complexity"],
            "royalty_rate": int(model["royalty_rate"] * 10000),  # Convert to basis points
            "deployment_timestamp": int(time.time()),
            "deployer": self.oinio_wallet if self.oinio_wallet else "0x0",
            "succession_ceremony": "OINIO December 2025",
            "documentation": "https://github.com/onenoly1010/pi-forge-quantum-genesis/blob/main/docs/SUCCESSION_CEREMONY.md",
            "license": "MIT",
            "tags": ["ethical-ai", "validation", "pi-network", "oinio"]
        }
        
        return metadata
    
    def upload_to_ipfs(self, metadata: Dict) -> str:
        """
        Upload model metadata to IPFS for decentralized storage
        
        Args:
            metadata: Model metadata dictionary
            
        Returns:
            IPFS hash (CID)
        """
        if self.dry_run:
            # Simulate IPFS upload
            ipfs_hash = f"Qm{'x' * 44}"  # Mock IPFS hash
            print(f"   [DRY RUN] Would upload to IPFS: {ipfs_hash}")
            return ipfs_hash
        
        # TODO: Implement actual IPFS upload
        # This would use an IPFS client library to upload the metadata
        # For now, returning a placeholder
        print("   üì§ Uploading to IPFS...")
        
        # Example implementation (commented out - requires ipfshttpclient):
        # import ipfshttpclient
        # client = ipfshttpclient.connect('/ip4/127.0.0.1/tcp/5001')
        # res = client.add_json(metadata)
        # ipfs_hash = res['Hash']
        
        ipfs_hash = f"Qm{'x' * 44}"  # Placeholder
        print(f"   ‚úÖ IPFS Hash: {ipfs_hash}")
        
        return ipfs_hash
    
    def deploy_smart_contract(self, model: Dict, ipfs_hash: str) -> Dict:
        """
        Deploy MR-NFT smart contract for the model
        
        Args:
            model: Model definition
            ipfs_hash: IPFS hash of metadata
            
        Returns:
            Deployment result with transaction hash and contract address
        """
        if self.dry_run:
            # Simulate deployment
            tx_hash = f"0x{'1234567890abcdef' * 4}"
            contract_address = f"0x{'abcdef123456' * 3}{'0' * 4}"
            
            print(f"   [DRY RUN] Would deploy contract")
            print(f"   [DRY RUN] Transaction: {tx_hash}")
            print(f"   [DRY RUN] Contract: {contract_address}")
            print(f"   [DRY RUN] Gas Limit: {self.gas_limit}")
            print(f"   [DRY RUN] Cost: {model['deployment_cost']} PI")
            
            # Simulate waiting for confirmation
            print("   [DRY RUN] Waiting for confirmation...")
            time.sleep(2)  # Simulate network delay
            print("   [DRY RUN] Confirmed!")
            
            return {
                "success": True,
                "tx_hash": tx_hash,
                "contract_address": contract_address,
                "gas_used": self.gas_limit // 2,
                "cost": model["deployment_cost"],
                "block_number": 12345678,
                "timestamp": int(time.time())
            }
        
        # TODO: Implement actual Pi Network deployment
        # This would use Pi Network SDK to deploy the contract
        print("   üìù Deploying smart contract...")
        
        # Example implementation (commented out - requires pi-network SDK):
        # from pi_network import create_client
        # client = create_client()
        # 
        # # Prepare deployment transaction
        # tx = client.deploy_contract(
        #     factory_address=self.factory_contract,
        #     model_name=model["name"],
        #     royalty_rate=int(model["royalty_rate"] * 10000),
        #     metadata_uri=f"ipfs://{ipfs_hash}",
        #     gas_limit=self.gas_limit
        # )
        # 
        # # Wait for confirmation
        # receipt = client.wait_for_transaction(tx.hash, timeout=300)
        # 
        # return {
        #     "success": receipt.status == 1,
        #     "tx_hash": receipt.transactionHash.hex(),
        #     "contract_address": receipt.contractAddress,
        #     "gas_used": receipt.gasUsed,
        #     "cost": model["deployment_cost"],
        #     "block_number": receipt.blockNumber,
        #     "timestamp": int(time.time())
        # }
        
        raise NotImplementedError("Pi Network deployment not yet implemented. Use --dry-run for testing.")
    
    def mint_mrnft(self, contract_address: str, model: Dict) -> Dict:
        """
        Mint the initial MR-NFT token for the model
        
        Args:
            contract_address: Deployed contract address
            model: Model definition
            
        Returns:
            Minting result with token ID
        """
        if self.dry_run:
            print(f"   [DRY RUN] Would mint NFT at {contract_address}")
            return {
                "success": True,
                "token_id": 1,
                "owner": self.oinio_wallet if self.oinio_wallet else "0x0"
            }
        
        # TODO: Implement actual NFT minting
        print("   üí∞ Minting MR-NFT...")
        raise NotImplementedError("NFT minting not yet implemented. Use --dry-run for testing.")
    
    def verify_deployment(self, result: Dict, model: Dict) -> bool:
        """
        Verify that deployment was successful
        
        Args:
            result: Deployment result
            model: Model definition
            
        Returns:
            True if verification passed
        """
        if self.dry_run:
            print(f"   [DRY RUN] Would verify deployment")
            return True
        
        # TODO: Implement verification
        # - Check contract exists at address
        # - Verify royalty rate is correct
        # - Test inference call
        # - Confirm ownership
        
        print("   ‚úÖ Deployment verified")
        return True
    
    def deploy_model(self, model: Dict) -> Dict:
        """
        Deploy a single model through the complete pipeline
        
        Args:
            model: Model definition
            
        Returns:
            Complete deployment result
        """
        print(f"\n{'='*60}")
        print(f"Deploying: {model['name']}")
        print(f"Royalty: {model['royalty_rate']*100}% | Cost: {model['deployment_cost']:,} PI")
        print(f"{'='*60}")
        
        try:
            # Step 1: Prepare metadata
            print("üìã Step 1: Preparing metadata...")
            metadata = self.prepare_model_metadata(model)
            
            # Step 2: Upload to IPFS
            print("üì§ Step 2: Uploading to IPFS...")
            ipfs_hash = self.upload_to_ipfs(metadata)
            
            # Step 3: Deploy smart contract
            print("üìù Step 3: Deploying smart contract...")
            deployment_result = self.deploy_smart_contract(model, ipfs_hash)
            
            if not deployment_result["success"]:
                raise Exception("Contract deployment failed")
            
            # Step 4: Mint NFT
            print("üí∞ Step 4: Minting MR-NFT...")
            mint_result = self.mint_mrnft(
                deployment_result["contract_address"],
                model
            )
            
            # Step 5: Verify
            print("üîç Step 5: Verifying deployment...")
            verified = self.verify_deployment(deployment_result, model)
            
            if not verified:
                raise Exception("Verification failed")
            
            # Compile complete result
            complete_result = {
                "model": model["name"],
                "status": "success",
                "metadata": metadata,
                "ipfs_hash": ipfs_hash,
                "deployment": deployment_result,
                "mint": mint_result,
                "verified": verified,
                "timestamp": datetime.utcnow().isoformat()
            }
            
            # Log result
            self.deployment_log.append(complete_result)
            
            print(f"\n‚úÖ {model['name']} deployed successfully!")
            print(f"   Contract: {deployment_result['contract_address']}")
            print(f"   Transaction: {deployment_result['tx_hash']}")
            print(f"   Token ID: {mint_result['token_id']}")
            print(f"   Cost: {deployment_result['cost']:,} PI")
            
            return complete_result
            
        except Exception as e:
            print(f"\n‚ùå Deployment failed: {str(e)}")
            error_result = {
                "model": model["name"],
                "status": "failed",
                "error": str(e),
                "timestamp": datetime.utcnow().isoformat()
            }
            self.deployment_log.append(error_result)
            return error_result
    
    def deploy_all_models(self, models: List[Dict]) -> List[Dict]:
        """
        Deploy all models sequentially
        
        Args:
            models: List of model definitions
            
        Returns:
            List of deployment results
        """
        print(f"\nüöÄ Starting deployment of {len(models)} models")
        print(f"   Mode: {'DRY RUN' if self.dry_run else 'LIVE'}")
        print(f"   Network: {self.pi_network_mode.upper()}")
        
        total_cost = sum(m["deployment_cost"] for m in models)
        print(f"   Total Cost: {total_cost:,} PI")
        
        if not self.dry_run and self.pi_network_mode == 'mainnet':
            print("\n‚ö†Ô∏è  WARNING: This will spend REAL PI on mainnet!")
            print(f"   Total: {total_cost:,} PI will be spent")
        
        results = []
        for i, model in enumerate(models, 1):
            print(f"\n[{i}/{len(models)}] Deploying {model['name']}...")
            result = self.deploy_model(model)
            results.append(result)
            
            # Add delay between deployments to avoid rate limiting
            if i < len(models):
                print("\n‚è≥ Waiting 5 seconds before next deployment...")
                time.sleep(5)
        
        return results
    
    def generate_report(self) -> str:
        """
        Generate deployment report
        
        Returns:
            Formatted report string
        """
        successful = [r for r in self.deployment_log if r["status"] == "success"]
        failed = [r for r in self.deployment_log if r["status"] == "failed"]
        
        total_cost = sum(
            r["deployment"]["cost"] for r in successful if "deployment" in r
        )
        
        report = f"""
{'='*60}
DEPLOYMENT REPORT
{'='*60}

Mode: {'DRY RUN' if self.dry_run else 'LIVE'}
Network: {self.pi_network_mode.upper()}
Timestamp: {datetime.utcnow().isoformat()}

SUMMARY
-------
Total Models: {len(self.deployment_log)}
Successful: {len(successful)}
Failed: {len(failed)}
Total Cost: {total_cost:,} PI

SUCCESSFUL DEPLOYMENTS
---------------------
"""
        for result in successful:
            if "deployment" in result:
                report += f"""
Model: {result['model']}
  Contract: {result['deployment']['contract_address']}
  Transaction: {result['deployment']['tx_hash']}
  Token ID: {result['mint']['token_id']}
  Cost: {result['deployment']['cost']:,} PI
  IPFS: {result['ipfs_hash']}
"""
        
        if failed:
            report += "\nFAILED DEPLOYMENTS\n------------------\n"
            for result in failed:
                report += f"\nModel: {result['model']}\n  Error: {result['error']}\n"
        
        report += f"\n{'='*60}\n"
        
        return report
    
    def save_deployment_log(self, filename: str = "deployment_log.json"):
        """Save deployment log to JSON file"""
        with open(filename, 'w') as f:
            json.dump(self.deployment_log, f, indent=2)
        print(f"\nüíæ Deployment log saved to: {filename}")


def main():
    """Main entry point"""
    parser = argparse.ArgumentParser(
        description="Deploy the six seed MR-NFT models to Pi Network",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Dry run (safe testing)
  python scripts/seed_first_six_models.py --dry-run
  
  # Deploy single model
  python scripts/seed_first_six_models.py --model "Ethics Validator" --execute
  
  # Deploy all six models (requires confirmation)
  python scripts/seed_first_six_models.py --execute-all --confirm
  
  # Deploy with custom gas limit
  python scripts/seed_first_six_models.py --execute-all --confirm --gas-limit 3000000

For full documentation, see: docs/DEPLOYMENT_CHECKLIST.md
        """
    )
    
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="Simulate deployment without spending PI"
    )
    
    parser.add_argument(
        "--model",
        type=str,
        help="Deploy single model by name (e.g., 'Ethics Validator')"
    )
    
    parser.add_argument(
        "--execute",
        action="store_true",
        help="Execute deployment (required for single model)"
    )
    
    parser.add_argument(
        "--execute-all",
        action="store_true",
        help="Deploy all six models"
    )
    
    parser.add_argument(
        "--confirm",
        action="store_true",
        help="Confirm you want to spend PI (required for mainnet)"
    )
    
    parser.add_argument(
        "--gas-limit",
        type=int,
        default=2_000_000,
        help="Gas limit for transactions (default: 2,000,000)"
    )
    
    parser.add_argument(
        "--output",
        type=str,
        default="deployment_log.json",
        help="Output file for deployment log (default: deployment_log.json)"
    )
    
    args = parser.parse_args()
    
    # Validation
    if not args.dry_run and not args.execute and not args.execute_all:
        parser.error("Must specify --dry-run, --execute, or --execute-all")
    
    if args.execute_all and not args.model is None:
        parser.error("Cannot use --execute-all with --model (choose one)")
    
    if args.execute and not args.model:
        parser.error("--execute requires --model to specify which model to deploy")
    
    # Safety check for mainnet
    pi_network_mode = os.getenv('PI_NETWORK_MODE', 'testnet')
    if not args.dry_run and pi_network_mode == 'mainnet' and not args.confirm:
        print("‚ùå Error: Mainnet deployment requires --confirm flag")
        print("   This ensures you understand real PI will be spent.")
        sys.exit(1)
    
    # Create deployer
    deployer = ModelDeployer(dry_run=args.dry_run, gas_limit=args.gas_limit)
    
    # Execute deployment
    try:
        if args.model:
            # Deploy single model
            model = next((m for m in SEED_MODELS if m["name"] == args.model), None)
            if not model:
                print(f"‚ùå Error: Model '{args.model}' not found")
                print(f"   Available models: {', '.join(m['name'] for m in SEED_MODELS)}")
                sys.exit(1)
            
            result = deployer.deploy_model(model)
            
        elif args.execute_all:
            # Deploy all models
            results = deployer.deploy_all_models(SEED_MODELS)
        
        # Generate and print report
        report = deployer.generate_report()
        print(report)
        
        # Save deployment log
        deployer.save_deployment_log(args.output)
        
        # Exit with appropriate code
        failed = [r for r in deployer.deployment_log if r["status"] == "failed"]
        if failed:
            print(f"\n‚ö†Ô∏è  Warning: {len(failed)} deployment(s) failed")
            sys.exit(1)
        else:
            print("\n‚úÖ All deployments successful!")
            sys.exit(0)
            
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è  Deployment interrupted by user")
        deployer.save_deployment_log(args.output)
        sys.exit(1)
    except Exception as e:
        print(f"\n‚ùå Fatal error: {str(e)}")
        deployer.save_deployment_log(args.output)
        sys.exit(1)


if __name__ == "__main__":
    main()

